{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c9fe95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 08:16:15.241688: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-24 08:16:15.241706: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model\n",
    "import keras.backend as K\n",
    "\n",
    "from evaluation_metrics import r_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1702a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to project data\n",
    "data_path = '/recsys/data/spotify/s2v_data/'\n",
    "# Path to trian and test file\n",
    "pickle_file = 'sentences_and_maps.pickle'\n",
    "# Path to Word2Vec embeddings\n",
    "vectors_file = 'song_vectors.kv'\n",
    "# Path to encoder dataset which we create here if it doesnt exist\n",
    "encoder_dataset = 'encoder_dataset.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628f8cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + pickle_file, 'rb') as handle:\n",
    "    train, test, track_data_map, reverse_track_lookup = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aaeb5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [val.split() for val in train]\n",
    "test = [val.split() for val in test]\n",
    "\n",
    "wv = KeyedVectors.load(data_path + vectors_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8d77d",
   "metadata": {},
   "source": [
    "## Dataset creation\n",
    "- Dataset is created as (song1_embedding, song2_embedding) for songs found one after another if both sonds have embeddings\n",
    "- We created 2 datasets, 1 of 500K examples and the other of 1M examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d53ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating train and validation sets\n",
    "# taking 500 for train, 50 for validation\n",
    "try:\n",
    "    with open(data_path + encoder_dataset, 'rb') as handle:\n",
    "        train_in, train_out, val_in, val_out = pickle.load(handle)\n",
    "\n",
    "except Exception as e:   \n",
    "    print('Loading failed: {}'.format(e))\n",
    "    print('Creating dataset')\n",
    "    train_in, train_out, val_in, val_out = [], [], [], []\n",
    "\n",
    "    train_size = 0\n",
    "    validation_size = 0\n",
    "    add_to_train = True\n",
    "\n",
    "    for pl in train:\n",
    "        prev_song = None\n",
    "\n",
    "        if len(train_in) >= 1000000:\n",
    "            add_to_train = False\n",
    "        if len(val_in) >= 100000:\n",
    "            break\n",
    "\n",
    "        for s in pl:\n",
    "            if s in wv.index_to_key:\n",
    "                if prev_song:\n",
    "                    if add_to_train:\n",
    "                        train_in.append(wv.get_vector(prev_song))\n",
    "                        train_out.append(wv.get_vector(s))\n",
    "                    else:\n",
    "                        val_in.append(wv.get_vector(prev_song))\n",
    "                        val_out.append(wv.get_vector(s))\n",
    "\n",
    "                prev_song = s\n",
    "            else:\n",
    "                prev_song = None\n",
    "\n",
    "    data = train_in, train_out, val_in, val_out\n",
    "    with open(data_path + encoder_dataset, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2a892ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_in, train_out= np.array(train_in), np.array(train_out)\n",
    "val_in, val_out = np.array(val_in), np.array(val_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59621da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_in = train_in.reshape(train_in.shape[0], 64, 1)\n",
    "train_out = train_out.reshape(train_in.shape[0], 64, 1)\n",
    "val_in = val_in.reshape(val_in.shape[0], 64, 1)\n",
    "val_out = val_out.reshape(val_in.shape[0], 64, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4306650d",
   "metadata": {},
   "source": [
    "## Model\n",
    "- The model is an Encoder Decoder model\n",
    "- The input is a song embedding\n",
    "- The output is also a song embedding\n",
    "\n",
    "- The target is given a song, predict the embedding of the next song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5976221a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 08:16:30.371843: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-02-24 08:16:30.371884: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (kal-syl): /proc/driver/nvidia/version does not exist\n",
      "2022-02-24 08:16:30.372244: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "class ProdEmbToEmb(Model):\n",
    "  def __init__(self):\n",
    "    super(ProdEmbToEmb, self).__init__()\n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Input(shape=(64, 1)),\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(32, activation='selu'),\n",
    "      layers.Dropout(0.2),\n",
    "      layers.Dense(32, activation='selu')])\n",
    "\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(32, activation='selu'),\n",
    "      layers.Dropout(0.3),\n",
    "      layers.Dense(64, activation='selu'),\n",
    "      layers.Dropout(0.3),\n",
    "      layers.Dense(64, activation='linear'),\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "\n",
    "autoencoder = ProdEmbToEmb()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8864bd07",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "prediction is done as follows:\n",
    "- Given a song we encode it end decode it using the model\n",
    "- Then we search for the nearest embeddings to the decoded vector\n",
    "- We recommend the song with the nearest embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3b241c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_similarity_to_decoding(s_sample, k, wv):\n",
    "    songs_in_dict = []\n",
    "    all_results = []\n",
    "    for s in s_sample:\n",
    "        if s in wv.key_to_index:\n",
    "            songs_in_dict.append(s)\n",
    "    \n",
    "    if songs_in_dict:\n",
    "        for s in songs_in_dict:\n",
    "            vec = wv.get_vector(s).reshape(1, 64, 1)\n",
    "            encoded_vec = autoencoder.encoder(vec).numpy()\n",
    "            decoded_vec = autoencoder.decoder(encoded_vec).numpy()\n",
    "            temp_results = wv.similar_by_vector(decoded_vec[0], topn=int(k / len(songs_in_dict)))\n",
    "            all_results.extend(temp_results)\n",
    "        \n",
    "        return all_results[:100]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b43f8e1",
   "metadata": {},
   "source": [
    "## Hyper Parameter tunning \n",
    "The parameters we tune are:\n",
    "- optimizer\n",
    "- loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa55958c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "15627/15627 [==============================] - 17s 1ms/step - loss: 1.5440 - val_loss: 1.4452\n",
      "Epoch 2/5\n",
      "15627/15627 [==============================] - 17s 1ms/step - loss: 1.5094 - val_loss: 1.4425\n",
      "Epoch 3/5\n",
      "15627/15627 [==============================] - 17s 1ms/step - loss: 1.5042 - val_loss: 1.4357\n",
      "Epoch 4/5\n",
      "15627/15627 [==============================] - 18s 1ms/step - loss: 1.5017 - val_loss: 1.4365\n",
      "Epoch 5/5\n",
      "15627/15627 [==============================] - 17s 1ms/step - loss: 1.4996 - val_loss: 1.4322\n",
      "Epoch 1/5\n",
      "15627/15627 [==============================] - 19s 1ms/step - loss: -0.5253 - val_loss: -0.5786\n",
      "Epoch 2/5\n",
      "15627/15627 [==============================] - 18s 1ms/step - loss: -0.5442 - val_loss: -0.5845\n",
      "Epoch 3/5\n",
      "15627/15627 [==============================] - 17s 1ms/step - loss: -0.5483 - val_loss: -0.5859\n",
      "Epoch 4/5\n",
      "15627/15627 [==============================] - 18s 1ms/step - loss: -0.5505 - val_loss: -0.5875\n",
      "Epoch 5/5\n",
      "15627/15627 [==============================] - 18s 1ms/step - loss: -0.5522 - val_loss: -0.5884\n",
      "Epoch 1/5\n",
      "15627/15627 [==============================] - 19s 1ms/step - loss: 1.6636 - val_loss: 1.4966\n",
      "Epoch 2/5\n",
      "15627/15627 [==============================] - 19s 1ms/step - loss: 1.5571 - val_loss: 1.4738\n",
      "Epoch 3/5\n",
      "15627/15627 [==============================] - 19s 1ms/step - loss: 1.5380 - val_loss: 1.4620\n",
      "Epoch 4/5\n",
      "15627/15627 [==============================] - 19s 1ms/step - loss: 1.5274 - val_loss: 1.4561\n",
      "Epoch 5/5\n",
      "15627/15627 [==============================] - 19s 1ms/step - loss: 1.5207 - val_loss: 1.4502\n",
      "Epoch 1/5\n",
      "15627/15627 [==============================] - 20s 1ms/step - loss: -0.3996 - val_loss: -0.5098\n",
      "Epoch 2/5\n",
      "15627/15627 [==============================] - 20s 1ms/step - loss: -0.4718 - val_loss: -0.5336\n",
      "Epoch 3/5\n",
      "15627/15627 [==============================] - 20s 1ms/step - loss: -0.4919 - val_loss: -0.5492\n",
      "Epoch 4/5\n",
      "15627/15627 [==============================] - 20s 1ms/step - loss: -0.5035 - val_loss: -0.5568\n",
      "Epoch 5/5\n",
      "15627/15627 [==============================] - 20s 1ms/step - loss: -0.5103 - val_loss: -0.5607\n",
      "Epoch 1/5\n",
      "15627/15627 [==============================] - 22s 1ms/step - loss: 1.5763 - val_loss: 1.4507\n",
      "Epoch 2/5\n",
      "15627/15627 [==============================] - 20s 1ms/step - loss: 1.5111 - val_loss: 1.4409\n",
      "Epoch 3/5\n",
      "15627/15627 [==============================] - 20s 1ms/step - loss: 1.5031 - val_loss: 1.4343\n",
      "Epoch 4/5\n",
      "15627/15627 [==============================] - 20s 1ms/step - loss: 1.4979 - val_loss: 1.4285\n",
      "Epoch 5/5\n",
      "15627/15627 [==============================] - 20s 1ms/step - loss: 1.4941 - val_loss: 1.4258\n",
      "Epoch 1/5\n",
      "15627/15627 [==============================] - 21s 1ms/step - loss: -0.5096 - val_loss: -0.5750\n",
      "Epoch 2/5\n",
      "15627/15627 [==============================] - 21s 1ms/step - loss: -0.5374 - val_loss: -0.5811\n",
      "Epoch 3/5\n",
      "15627/15627 [==============================] - 21s 1ms/step - loss: -0.5423 - val_loss: -0.5836\n",
      "Epoch 4/5\n",
      "15627/15627 [==============================] - 21s 1ms/step - loss: -0.5451 - val_loss: -0.5851\n",
      "Epoch 5/5\n",
      "15627/15627 [==============================] - 21s 1ms/step - loss: -0.5473 - val_loss: -0.5866\n"
     ]
    }
   ],
   "source": [
    "params_df = pd.DataFrame(columns=['time', 'optimizer', 'loss', 'R-Precision'])\n",
    "\n",
    "optimizers = ['adam', 'sgd', 'adamax']\n",
    "\n",
    "_losses = [(losses.MeanSquaredError, 'MSE'), (losses.CosineSimilarity, 'Cosine_Similarity')]\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    for loss in _losses:\n",
    "        start = time.time()\n",
    "        \n",
    "        autoencoder = ProdEmbToEmb()\n",
    "        autoencoder.compile(optimizer=optimizer, loss=loss[0]())\n",
    "\n",
    "        autoencoder.fit(train_in, train_out,\n",
    "                        epochs=5,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(val_in, val_out))\n",
    "\n",
    "        similarity_prediction = [predict_using_similarity_to_decoding(sample[:3], 100, wv) for sample in test[50000:52000]]\n",
    "        test_found = [val for val, tester in zip(test[50000:52000], similarity_prediction) if tester]\n",
    "        similarity_prediction = [val for val in similarity_prediction if val]\n",
    "\n",
    "        params_df = params_df.append({'time': (time.time() - start) / 60,\n",
    "                                      'optimizer': optimizer,\n",
    "                                      'loss': loss[1],\n",
    "                                      'R-Precision': r_precision(test_found, similarity_prediction)},\n",
    "                                     ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b3772ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>loss</th>\n",
       "      <th>R-Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.567158</td>\n",
       "      <td>adam</td>\n",
       "      <td>MSE</td>\n",
       "      <td>0.094779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.744873</td>\n",
       "      <td>adam</td>\n",
       "      <td>Cosine_Similarity</td>\n",
       "      <td>0.064768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.754101</td>\n",
       "      <td>sgd</td>\n",
       "      <td>MSE</td>\n",
       "      <td>0.096905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.857194</td>\n",
       "      <td>sgd</td>\n",
       "      <td>Cosine_Similarity</td>\n",
       "      <td>0.061437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.923372</td>\n",
       "      <td>adamax</td>\n",
       "      <td>MSE</td>\n",
       "      <td>0.096330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.059028</td>\n",
       "      <td>adamax</td>\n",
       "      <td>Cosine_Similarity</td>\n",
       "      <td>0.069093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       time optimizer               loss  R-Precision\n",
       "0  2.567158      adam                MSE     0.094779\n",
       "1  2.744873      adam  Cosine_Similarity     0.064768\n",
       "2  2.754101       sgd                MSE     0.096905\n",
       "3  2.857194       sgd  Cosine_Similarity     0.061437\n",
       "4  2.923372    adamax                MSE     0.096330\n",
       "5  3.059028    adamax  Cosine_Similarity     0.069093"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
